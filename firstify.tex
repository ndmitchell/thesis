
%include thesis.fmt

\chapter{First-Order Reduction}

This chapter deals with the reduction from a higher-order Core language to a primarily first-order Core language. Our motivation is that the Catch analysis tool (see Chapter \ref{chp:catch}) is designed to work only upon a first-order language, but our method may have use for other analysis tools, particularly termination checking \cite{serini:term,someone_else,someone_else}. The transformations presented in this chapter are all semantics preserving, but do \textit{not} all preserve sharing. As such, these transformations may not be suitable for optimisation, but are perfectly acceptable for analysis methods.


\section{Higher-Order Elements}

A program can be said to be higher-order if at runtime the program generates and manipulates functional values. In our Core language as currently defined, the expression |id 1| first evaluates |id| to a functional value, then applies the argument |1|. For our higher-order classification we treat a top-level function symbol and all statically given application arguments as one unit, meaning |id 1| is not a functional value.

\begin{example}
\label{ex:ho_elements}
The following two functions can be thought of as having functional elements:

\begin{code}
even = not . odd

map f x = case  x of
                []    -> []
                y:ys  -> f y : map f ys
\end{code}

The |even| function passes |not| and |odd| to the function |(.)|, both of which are functional values. In addition, the |(.)| function, when applied to only two arguments, returns a functional value. The definition of |map| does not create any functional values, but applies the variable |f| to an argument, suggesting that |f| is a functional value.
\end{example}

We classify higher-order elements as either \textit{creating} or \textit{applying} functional values.


\subsection{Creating Functional Values}

The most obvious way to create a functional value in our Core language is with an explicit lambda expression. The other way is to \textit{curry} or \textit{partially apply} a function, by passing fewer arguments than the arity of the function.

\begin{example}
\begin{code}
example1 = (\x -> x) 42

example2 xs = map id xs
\end{code}

Here |example1| contains an explicit lambda, which is a functional value. In |example2| the function |id| has an arity of 1, but is not given any arguments, and hence is a functional value.
\end{example}

It is possible for a program to have either one of these syntatic conditions, but at runtime, not create any functional values. Consider the following code:

\begin{code}
main = case  True of
             True   -> 1
             False  -> (\x -> x) 2
\end{code}

Here the |True| branch will always be taken due to the known case scrutinee. The |False| branch would create a functional value if taken, but as it will never be taken, this program will never create a functional value. This situation can be thought of as having functional elements in dead code.


\subsection{Applying Functional Values}

There are two ways to make use of functional values. The first is to apply an argument to an expression which is not a constructor or a top-level function. The second is to give a function more arguments than its associated arity.

\begin{example}
\begin{code}
example1 f = f 42

example2 = even 42
\end{code}

In |example1| the argument |f| is applied to the value 42, suggesting that |f| is functional. In |example2|, |even| is applied to one argument. Using the definition of |even| from Example \ref{ex:ho_elements}, with arity 0, this causes |even| to be given more arguments than its arity.
\end{example}

As before, we can construct an example where a functional value is applied, but which never occurs at runtime due to dead code. Another way of constructing an apparent application of a functional value is using |undefined| or a call to |error|.

\begin{example}
\begin{code}
main = error "bang" 42
\end{code}

Here the |error| primitive is being given two arguments, even though at first glance it appears to only take one. The is possible because the type of error is |String -> alpha|, where |alpha| is completely unconstrained and can be anything, including a functional type.
\end{example}


\subsection{First-Order Core}
\label{sec:first_order_restrictions}

We define a program to be first-order if it contains no expressions of the two types identified as creating functional values. In order to allow the property to be calculated statically, we ignore the issue of dead code.

There exist programs, or fragments of code, which cannot be reduced to first-order. We present several examples in turn, explaining why first-order reduction is not possible.

\begin{example}
\begin{code}
main = [id]
\end{code}

In this example, the |main| function returns a functional value inside a constructor. We cannot remove the functional value without changing the semantics of the |main| function, which is called from outside the our program, and hence cannot be altered. A related situation is:

\begin{code}
main = id
\end{code}

Here we can only reduce this program to first-order if we are allowed to increase the arity of |main| from zero to one. This situation occurs frequently in Haskell programs, whose |main| definition is typically of type |IO ()|. In the Yhc compiler, used to generate our Core language, the definition of |IO| is:

\begin{code}
newtype IO alpha = IO (World -> _E alpha)
\end{code}

At compilation time the |newtype| wrapper is removed, leaving a function from |World| to |_E alpha|. The |main| argument therefore takes a |World| parameter, before returning a first-order result. We permit the increasing of the arity of |main|.
\end{example}

\begin{example}
\begin{code}
main = id `seq` 42
\end{code}

Here a functional value (|id|) is passed to the primitive |seq|. As we are not able to peer inside the primitive, and must preserve its interface, we cannot remove this functional value. For most primitives, such as arithmetic operations, the types ensure that no functional values are passed as arguments. However, the |seq| primitive is of type |alpha -> beta -> beta|, allowing any type to be passed as either of the arguments, including functional values.

Another primitive which permits functional values is |primCatch :: alpha -> (beta -> alpha) -> alpha|. While |seq| merely permits functional values, |primCatch| actually \textit{requires} that the second argument is a functional value.
\end{example}

\begin{example}
\begin{code}
main f = f id
\end{code}

In this example, the |main| function takes a functional argument |f|, which is applied to |id| -- a functional value. Since the interface to |f| is outside the control of the code we are specialising, we cannot change its interface.
\end{example}

In all these examples, a functional value must be created as it is required by either the interface to a primitive, or the interface to the root function. In a similar manner, the root function may have a functional argument, or a primitive may return a functional value, resulting in a functional application.

If none of the above cases occurs, then it is always possible to remove all function values from a Core program. In the following sections we will assume that The root function returns a first-order value, and takes no parameters. Within these assumptions, then only necessary residual lambda expressions occur within primitive functions. One method for removing higher-order functions is Reynolds style defunctionalisation, which we detail first. Next we detail our method, which has some important differences from Reynold's method.


\section{Reynolds style defunctionalization}

Reynolds style defunctionalization \cite{reynolds:defunc} is the seminal method for generating a first-order equivalent of a higher-order program.

\begin{example}
\begin{code}
map f x = case  x of
                []      -> []
                (y:ys)  -> f y : map f ys
\end{code}

\noindent Defunctionalization works by creating a data type to represent all values that |f| may take anywhere in the whole program. For instance, it might be:

\ignore\begin{code}
data Function = Head | Tail

apply Head  x = head  x
apply Tail  x = tail  x

map f x = case  x of
                []    -> []
                y:ys  -> apply f a : map f as
\end{code}

\noindent Now all calls to |map head| are replaced by |map Head|.
\end{example}

This method naturally extends to partial application. To take a more complicated example, where higher-order functions are being used to store information:

\begin{example}
\begin{code}
type Map = String -> Int

new :: Map
new _ = 0

get :: String -> Map -> Int
get key mp = mp key

add :: String -> Int -> Map -> Map
add key val mp s = if s == key then val else get key mp

test = get "foo" (add "bar" 4 (add "baz" 2 new))
\end{code}

\noindent The above code creates a functional map, which uses a higher-order function to store a mapping from |String| to |Int|. The |add| function inserts a new key/value pair into the map. This is transformed with defunctionalization to:

\begin{code}
data Function  =  New
               |  Add3 String Int Function

apply  New                 x = new x
apply  (Add3 y_1 y_2 y_3)  x = add y_1 y_2 y_3 x

new _ = 0

get key mp = apply mp key

add key val mp s = if s == key then val else get key mp

test = get "foo" (Add3 "bar" 4 (Add3 "baz" 2 New))
\end{code}

Here we use the constructor |Add3| to represent the |add| function with three arguments pre-applied. Note that the |Function| data type now serves to store a linked-list of the values with |New| serving a similar role to |[]|, and |Add3| storing one key/value pair along with the remainder of the list.
\end{example}

We are unaware of any simple method for extending Reynold's style defunctionalisation to primitives, without changes the primitives to be aware of the |Ap| data type. Defunctionalized code is still type safe, but type checking would require a dependently typed language. The method is complete, removing all possible higher-order functions, and preserves space behaviour. The disadvantage is that the transformation essentially embeds a mini-interpreter for the original program into the new program. The flow control is complicated by the extra level of indirection.

Some uses of Reynold's method include as preprocessing step in a whole-program analysis \cite{grin,jhc} and as a step in a program transformation \cite{graham_hutton_calculating_an_exceptional_machine}.

\section{Our First-Order Reduction Method}

A natural desire would be to eliminate the higher-order aspects of a program, without introducing any new data structures. However, it is simple to show that this transformation is not possible. Given a program, we can remove all data structures by Church encoding \cite{church_encode}. If we then had a transformation which made the program first-order \textit{without} introducing any data, we would end up with a program without data or closures, which is therefore incapable of storing an unbounded amount of information. Since with higher-order functions we can implement a Turing machine \cite{turing:halting}, and without an unbounded store we cannot, such a transformation cannot exist.

Our reduction method proceeds in four steps, each removing some higher-order elements, without introducing any data structures. Each step is independently terminating, and are combined using a fixed point operator. Because our method terminates and does not introduce any data structures, it is necessarily incomplete -- but Reynold's method can be used afterwards.

Our method proceeds in four steps:

\begin{code}
firstify = lambdas +|+ simplify +|+ inline +|+ specialise
\end{code}

Each stage will be described separately. The overall control of the algorithm is given by the |(+||+)| operate, defined as:

\begin{code}
infixl +|+

(+|+) :: Eq alpha => (alpha -> alpha) -> (alpha -> alpha) -> alpha -> alpha
(+|+) f g x  = fix (fix x . y)

fix :: Eq alpha => (alpha -> alpha) -> alpha -> alpha
fix f x = if x == x2 then x else fix f x2
    where x2 = f x
\end{code}

The |(+||+)| operator applies the first argument until it reaches a fixed point, then applies the second argument. If the second argument changes the value, the first argument is tried again until a fixed point is achieved. This formulation has several important properties:

\begin{description}
\item[Idempotent in each function] After the operation has completed, applying either |f| or |g| will not change the value.

\begin{code}
forall f g x `o` let r = (+|+) f g x in f r == r && g r == r
\end{code}

\item[Idempotent] The operation as a whole is idempotent.

\begin{code}
forall f g x `o` let r = (+|+) f g x in r == (+|+) f g r
\end{code}

\item[Function ordering] The function |f| will have reached a fixed point before the function |g| is applied.
\end{description}

The final property allows us to overlap the application sites between the two arguments, but guarantee the first will always be chosen. The other two properties ensure that when the operation finishes there will be no further sites where application could occur.

The operator is left associative, meaning that the code can be rewritten with explicit bracketing as:

\begin{code}
firstify = ((lambdas +|+ simplify) +|+ inline) +|+ specialise
\end{code}

Within this chain we guarantee that the end result will be idempotent with respect to any of the functions, and before any function is invoked, all those to the left of it will be idempotent.

The operator |(+||+)| is written for clarity, not for speed. If the first argument is idempotent on its own, then additional unnecessary work is performed. In the case of chaining operators, the left function is guaranteed to be idempotent in all but the first case, so much computation is duplicated. The |(+||+)| operator also checks for global equality, when typically operations will only operate within some locality, which could be exploited.

We now describe each of the stages in the algorithm separately.

\subsection{Lambda Insertion (|lambdas|)}

The first stage removes all occurrences of partial application, replacing them with explicit lambda expressions.

\begin{examplerevisit}{\ref{ex:ho_elements}}
\begin{code}
even = \x -> (.) (\y -> not y) (z -> odd z) x
\end{code}

Here the |even| function, which previously had three instances of partial application, has three lambda expressions inserted. Now each function is applied to a number of arguments equal to its arity. In this particular instance, the introduction of lambda expressions changes the arity of |even| from 0 to 1, possibly requiring applications of |even| to become partially applied.
\end{examplerevisit}

For each partially applied function, a lambda expression is inserted to ensure that the function is now given at least as many arguments as its associated arity. This step trades one form of function creation for another form, but has the advantage of making functional values more explicit.

\subsection{Simplification (|simplify|)}

\begin{figure}
\renewcommand{\f}[2]{\vspace{-7mm} #2 & (#1) \\}

\begin{flushright}
\begin{tabular}{p{8cm}r}
\f{case-lam}{
\begin{code}
case x of {... ; c vs_ -> \v -> y ; ...}
    => \v' -> case x of {... ; c vs_ -> \v -> y ; ...} v'
\end{code}}

\f{bind-lam}{
\begin{code}
let v = \v' -> x in y
    => y[v/ \v' -> x]
\end{code}}


\f{let-lam}{
\begin{code}
let v = x in \v' -> y
    => \v' -> let v = x in y
\end{code}}
\end{tabular}
\end{flushright}
\caption{Lambda Simplification rules.}
\label{fig:lambda_simplify}
\end{figure}

The second stage attempts to move lambda's upwards until they form part of the arity of a function. This stage makes use of the general simplification rules (Figure \ref{fig:simplify}), along with some additional rules which deal with lambda expressions, given in Figure \ref{fig:lambda_simplify}.

The rule (case-lam) lifts a lambda out from within a case alternative to outside the case value. The (bind-lam) rule inlines a lambda bound in a let expression. The (let-lam) rule is the one responsible for the largest loss of sharing, promoting a lambda expression outside a let.

\begin{example}
\begin{code}
f x = let i = expensive x
      in \j -> i + j

main xs = map (f 1) xs
\end{code}

In the above example, |expensive 1| is computed once and saved. Every application of the functional argument within |map| performs a single |(+)| operation. After applying the (let-lam) rule we get:

\begin{code}
f x j = let i = expensive x
        in i + j
\end{code}

Now |expensive| will be recomputed for every element in |xs| -- potentially a very severe speed penalty.
\end{example}

The loss of sharing is not purely theoretical, the Uniplate library makes use of a let within a lambda (see \S\ref{sec:optimise_playdata}) to keep a scoreboard. The (let-lam) rule would make the scoreboard mechanism a severe performance penalty.

\subsection{Inlining (|inline|)}

After inserting additional lambda expressions, and performing simplification, there may still be residual lambda expressions which are not at the top level of a function. The simplification rules will promote lambda expressions up above let expressions and case expressions. With the additional rules, the only residual lambda expressions will be as arguments to either a constructor, or a function. The inline stage deals with lambda expressions which are residual to a constructor at the top level, the specialise stage deals with all remaining lambda expressions.

\begin{example}
Given a program making use of type class dictionaries, as detailed in \S\ref{sec:dictionary_transformation}, we may end up with the following code:

\begin{code}
eqInt = (\x y -> primEqInt x y, \x y -> primNeqInt x y)
(==) d x y = d x y

main = (==) eqInt 1 2
\end{code}

Here the lambda expression including |primEqInt| is an argument to the constructor |(,)| in the top-level of |eqInt|. The inline stage will inline the |eqInt| function anywhere it occurs, resulting in:

\begin{code}
(==) (a,b) x y = a x y

main = (==) (\x y -> primEqInt x y, \x y -> primNeqInt x y) 1 2
\end{code}

The lambda expressions are still present, but hopefully can be removed using other techniques.
\end{example}

The general rule is to inline all functions containing a subexpression |alpha|, where: |alpha| is a constructor application with one argument which is a lambda expression; and |alpha| is not a subexpression of any argument to a function application.

\begin{example}
\begin{code}
yes0 = [\x -> x]
yes1 = Maybe [\x -> x]
yes2 = let y = 1 in [\x -> x]
no0 = \x -> id x
no1 = [id (\x -> x)]
no2 = id [\x -> x]
\end{code}

In this example, we would inline the |yes| functions, but none of the others. In |no0| there is no constructor, so the first condition does not apply. In |no1| there is a constructor, but the lambda expression is not an argument to the constructor. In |no2| there is a lambda expression as an argument to a constructor, but that subexpression is an argument to a function application.
\end{example}

The |inline| rule deals with situations where functions evaluate to a data structure containing functional values. This situation occurs regularly with the standard dictionary implementation, but rarely in other situations. The inline rule does not actually remove functional values, but can bring their use and creation closer together, and thus helps them be removed.

There may be some loss of sharing with the inline rule, if a CAF is inlined.

\subsection{Specialisation (|special|)}

The original Catch tool \cite{me:catch_tfp} uses specialisation to remove higher-order functions. For each application of a function to functional arguments, a specialised variant is created, and used where applicable. The process follows the same pattern as constructor specialisation \cite{spj:specconstr}, but applied where function arguments are partially applied functions, rather than known constructors. Examples of common functions whose applications can usually be made first-order include |map|, |filter|, |foldr| and |foldl|.

The specialisation transformation makes use of \textit{templates}. A template is an expression where some sub-expressions are omitted, denoted by an underscore. The process of specialisation proceeds as follows:

\begin{enumerate}
\item Find all functions which have functional arguments, and generate templates, omitting first-order components.
\item For each template, generate an associated function, specialised to the template.
\item For each subexpression matching a template, replace it with the associated function.
\end{enumerate}

\begin{example}
\begin{code}
main xs = map (\x -> x) xs

map f xs = case  xs of
                 []    -> []
                 y:ys  -> f y : map f ys
\end{code}

The specialisation first finds the application of |map| within |main|, and generates the template |map (\x -> x) _| -- omitting the |xs| which is not obviously a functional value. It then generates the name |map_id| for the template, and generates an appropriate function body. Next all calls matching the template are replaced with calls to |map_id|, including in the call to |map| within the freshly generated |map_id|.

\begin{code}
main xs = map_id xs

map_id xs = case  xs of
                  []    -> []
                  y:ys  -> y : map_id ys
\end{code}

The resulting code has no functional values within it.
\end{example}

\subsubsection{Generating Templates}

A template is generated if an expression is an application to a top-level function, whose arguments include a sub-expression which is a lambda expression. The template includes all sub-expressions whose removal would lead to higher-order elements, or which have free variables from a bound variable.

\begin{example}
\begin{code}
id (\x -> x)              => id (\x -> x)
id (Maybe (\x -> x))      => id (Maybe (\x -> x))
id (Maybe (\x -> x + 3))  => id (Maybe (\x -> x + _))
id (Maybe (\x -> x + y))  => id (Maybe (\x -> x + _))
\end{code}

In all three examples, the |id| function has an argument which has a lambda expression as a subexpression. In the final two cases, the |3| and |y| are not dependent on variables bound within the lambda, and are left as unspecified.  The |Maybe| and |+| functions are also not dependent on the bound variables, however their removal would require a functional argument as a parameter, so are left as part of the template.
\end{example}

\subsubsection{Generating Functions}

Given a template, to generate an associated function, a unique function name is allocated to the template. Each |_| within the template is assigned a free variable, as an argument to the new function, then the body is produced by unfolding the outer function symbol in the template once.

\begin{example}
\label{ex:map_id}
Following the |map (\x -> x) _| template from above, we can generate |v_1| as the unique free variable for the single |_| placeholder, and |map_id| as the function name:

\begin{code}
map_id v_1 = map (\x -> x) v_1
\end{code}

In the next step, we unfold the definition of map once:

\begin{code}
map_id v_1 = let  f   = \x -> x
                  xs  = v_1
             in   case  xs of
                        []    -> []
                        y:ys  -> f y : map f ys
\end{code}

Now the generation of the specialised variant is complete. To give an idea of how the final function is calculated, after the simplification rules introduced in Figure \ref{fig:lambda_simplify}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map (\x -> x) ys
\end{code}
\end{example}

\subsubsection{Using Templates}

After a function has been generated for each template, every expression matching a template can be replaced by a call to the new function. Every subexpression corresponding to an undecided element is passed as an argument. Continuing with the generated code from Example \ref{ex:map_id}, we end up with:

\begin{code}
map_id v_1 =  let  xs = v_1
              in   case  xs of
                         []    -> []
                         y:ys  -> y : map_id ys
\end{code}

We have now eliminated all the functional values from within this operation.


\section{Proof of Completeness}

The algorithm we have presented is complete, in the sense that if it terminates, there will be no remaining lambda expressions within the program, other than those identified bellow. We have already shown that it is impossible to have both completeness and not introduce data structures (which our algorithm does not), therefore it follows that our algorithm is \textit{not} terminating. We shall return to the issue of termination in \S\ref{sec:firstify_terminate}, but assuming that our algorithm does terminate, it is possible to show that there are no remaining functional values.

We originally identified three causes of incompleteness when converting a program to first-order, detailed in \S\ref{sec:first_order_restrictions}. Two of those can be removed by restricting the root function to take no functional arguments and return a first-order value. The only remaining necessary cause is passing a lambda expression to a primitive function. Our method introduces one final instance of incompleteness -- passing a lambda to an expression that evaluates to |undefined|, caused by either a call to the |error| primitive, or by non-termination.

We can prove that, other than the restrictions introduced above, there are no resultant functional values created. We assume for the purposes of this proof that all functions are non-primitive.

\newenvironment{lemma}[1]
    {\paragraph{Lemma:} #1 \textbf{Proof:} }
    {\hfill$\Box$}

\begin{lemma}{There is no partial application in the resultant code.}
After our algorithm terminates, the resultant code must be idempotent with respect to each of the four stages presented. The |lambdas| stage ensures there will be no partial application by inserting explicit lambda expressions.
\end{lemma}

\begin{lemma}{The root expression of each top-level function is not a lambda.}
If a top-level function is bound to a lambda expression, then the arity of the function is increased and the lambda is no longer the root expression. Another way of expressing this property is that all lambda expressions must have a parent expression.
\end{lemma}

\begin{lemma}{The parent of a lambda expression must be an application, applied to a variable.}
In all other cases, one of the rules will transform the expression. To enumerate the possible parent expressions:

\begin{description}
\item[lambda abstraction:] the desugaring rule for lambda abstractions would combine them to one lambda abstraction.
\item[let binding:] the (let-lam) and (bind-lam) rules applied as part of |simplify| ensure these expressions are removed.
\item[case expression:] the (case-lam) rule will remove any lambda abstractions from the alternatives of a case expression. The static typing in the original Core language ensures that a lambda cannot be the scrutinee of a case expression.
\item[application:] the (lam-app) rule ensures that the lambda abstraction must be one of the arguments to the application. We now enumerate all the possible applied values within the application:
    \begin{description}
    \item[constructor:] Either the expression is the child of a function application, in which case the |special| stage will remove it, or it is not, in which case the |inline| stage will remove it.
    \item[function:] The |special| stage will specialise the function with respect to the lambda abstraction.
    \item[application:] The application desugaring rules will combine the application with the current one, removing this situation.
    \item[let binding:] The (let-app) rule applies.
    \item[case expression:] The (case-app) rule applies.
    \end{description}

    The only possibility not covered is that there is an application whose applied expression is a variable.
\end{description}
\end{lemma}

\begin{lemma}{All applications of a variable to a lambda expression evaluate to |undefined|.}
Given an expression |v (\v' -> x)|, the variable |v| must evaluate to |undefined|. It is clear from the type rules of our Core language that the variable |v| must either evaluate to |undefined|, or to a lambda expression. If it evaluates to a lambda expression, there must be a lambda expression within the program, since we are assuming no primitive functions or functional arguments to the root function. If such a lambda exists, it must be itself under an application to a variable, since all other possibilities have been eliminated. Given many such enclosing |v| variables, we must evaluate one first. The first one we evaluate cannot evaluate to a lambda expression, as all lambda expressions must result from first evaluating a variable, and since this is the first variable to be evaluated, that is not possible. The only other case is that the variable evaluates to |undefined|.
\end{lemma}


\section{Proof of Correctness}

To prove the correctness of the whole argument it is sufficient to show the correctness of each step. The simplification rules are all standard within lazy Core languages. The inlining and specialisation stages are also based on well accepted transformations. the only interesting case, from a correctness perspective, is lambda insertion.

The rule for lambda insertion expands all partially applied functions so they are given sufficiently many arguments. Given a function |f| of arity $n$, if an application of |f| has fewer than $n$ arguments, an application and a lambda is inserted with $n$ arguments. In general, the insertion of a lambda and application is not semantics preserving:

\begin{code}
x => \v -> x v
\end{code}

Consider for instance |undefined `seq` 1|, which evaluates to |undefined|. Applying the lambda introduction rule we can replace the code with |(\v -> undefined v) `seq` 1|, which evaluates to |1|. However, a closely allied operation that is semantics preserving is:

\begin{code}
\v -> x => \v' -> (\v -> x) v'
\end{code}

The lambda insertion phase can be seen as first transforming the function |f| using the lambda insertion rule, then inlining the outer lambda into the application site. These operations are semantics preserving, and relatively standard.


\section{Proof of Termination}
\label{sec:firstify_terminate}

Our algorithm, as it stands, is not terminating. In order to ensure termination, it is necessary to bound both the inlining and specialisation stages. In this section we develop the termination criteria, by first looking at how non-termination may arise.


\subsection{Termination of Inlining}

The standard technique for dealing with the termination of inlining is to refuse to inline recursive functions \cite{spj:inlining}. In practice, for first order reduction, the non-recursive restriction is overly cautious and leaves residual lambda expressions. We first present a program which causes our method to non-terminate, then our criteria for ensuring termination.

\begin{example}
\begin{code}
f = (\x -> x) : f
\end{code}

The expression bound to |f| will repeatedly grow in size as inlining is applied, and will remain a candidate for inlining.
\end{example}

Our termination criteria permits inlining a function |f|, at all application sites within a function |g|, but only once per pair |(f,g)|. In the above example we would be permitted to inline |f| within the function |f| at all application sites (only one in this example), once. The resultant code would be:

\begin{code}
f = (\x -> x) : (\x -> x) : f
\end{code}

Any future attempts to inline |f| within this function would be disallowed, although |f| could be inlined within other functions. This termination criteria is sound, assuming all expressions are finite and there are a finite number of function symbols. Each inlining will occur at only a finite number of application sites, and prohibit that pair of function inlinings occurring in future. Given $n$ functions, there can only be $n^2$ possible inlining steps, each for possibly many application sites.


\subsection{Termination of Specialisation}

The termination of specialisation is more tricky than inlining. We permit the inlining 

A natural extension of specialisation is to take the fixed point, eliminating unsaturated expressions in generated functions. Unfortunately, such an algorithm would not terminate.

\begin{example}
\begin{code}
data Wrap a  =  Wrap (Wrap a)
             |  Value a

f x = f (Wrap x)
main = f (Value head)
\end{code}

In the first iteration, this would generate a version of |f| specialised to |Value head|. In the second iteration it would specialise |f| with respect to |Wrap (Value head)|, then in the third with |Wrap (Wrap (Value head))|. We would generate an infinite number of specialisations of |f|.
\end{example}

One simple way to prevent such non-termination is to have a bound on the number of specialisations. Another approach is to use a homeomorphic embedding. All functions relate to some original expression in the Core language, if the expression to be generated was a homeomorphic embedding of an already specialised expression, we can stop.

Using homeomorphic embedding on the previous example, we would generate the following specialised variants of |f (Value head)| and |f (Wrap (Value head))|. Upon attempting to generate the specialised variant |f (Wrap (Wrap (Value head)))| we would abort, with an embedding of |f (Wrap (Value head))|.





\section{Results}

Our preferred method for higher-order function removal is to apply specialisation and inlining interleaved. We have tried our method on the nofib suite, and have the following results.
