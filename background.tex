
%include thesis.fmt

\chapter{Background}
\label{chp:background}

In this chapter we introduce the background material and general notations used throughout the rest of this thesis. We start by introducing a Core language in \S\ref{sec:core}, then discuss the semantics of this Core language in \S\ref{sec:semantics}, including its sharing properties in \S\ref{sec:sharing}.

\section{Core Language}
\label{sec:core}

\begin{figure}
\begin{code}
prog  =  f |--> x

expr  =  v                                          {-" \text{  variable} "-}
      |  c                                          {-" \text{  constructor} "-}
      |  f                                          {-" \text{  function} "-}
      |  x y                                        {-" \text{  application} "-}
      |  \v -> x                                    {-" \text{  lambda abstraction} "-}
      |  let v = x in y                             {-" \text{  let binding} "-}
      |  case x of {p_1 -> y_1 ; ... ; p_n -> y_n}  {-" \text{  case expression} "-}

pat   =  c vs_
\end{code}

Where |v| ranges over variables, |c| ranges over constructors, |f| ranges over function names, |x| and |y| range over expressions and |p| ranges over patterns.
\caption{Core syntax}
\label{fig:core}
\end{figure}

The expression type of our Core language is given in Figure \ref{fig:core}. To specify a list of items of unspecified length we write either |x_1,...,x_n| or |xs_|. A program is a mapping of function names to expressions. Our Core language is higher order and lazy, but lacks much of the syntactic sugar found in Haskell. Pattern matching occurs only in case expressions; alternatives match only the top level constructor and are exhaustive, including an |error| alternative if necessary.

We allow a list of variables to appear in a lambda abstraction, a list of expressions to appear as the second argument of an application and a list of bindings to appear in a let. When a top-level function is bound to a lambda, we allow the variables to be moved to the left of the equals. The arity of a top-level function is the number of arguments in an associated lambda expressions. This syntactic sugar can be translated away using the following rules:

\begin{code}
x y ys_                  => (x y) ys_
\v vs_ -> x              => \v -> (\vs_ -> x)
let v = x ; binds_ in y  => let v = x in (let binds_ in y)
let v vs_ = x xs_ in y   => let v = x in (let vs_ = xs_ in y)
f vs_ = x                => f = \vs_ -> x
\end{code}

Some functions are used but lack corresponding definitions in the program. These are defined to be \textit{primitive}. They have some meaning to an underlying runtime system, but are not available for transformation. A primitive function may perform an action such as outputting a character to the screen, or may manipulate primitive numbers such as addition.

The primary difference between our Core language and GHC-Core \cite{ghc_core} is that our Core language is untyped. The Core is generated from well-typed Haskell, and is guaranteed not to fail with a type error. All the work we do could be implemented equally well in a typed Core language, but we prefer to work in an untyped language for simplicity of implementation. For describing data types we use the same notation as Haskell 98. One of the most common data types is the list, which can be defined as:

\begin{code}
data List alpha = Nil | Cons alpha (List alpha)
\end{code}

A list is either an empty list, or a cons cell which contains an element of the list type and the tail of the list. For example the list of 1,2,3 would be written |(Cons 1 (Cons 2 (Cons 3 Nil)))|. We allow the syntactic sugar of representing |Cons| as a right-associative infix application of |(:)| and |Nil| as |[]| -- allowing us to write |(1:2:3:[])|. We also permit |[1,2,3]|.

We allow |if| expressions, with the translation:

\begin{code}
if x then y else z => case x of {True -> y; False -> z}
\end{code}

\subsection{Generating Core}

In order to generate our Core language from the full Haskell language, we use the Yhc compiler \cite{yhc}, a fork of nhc \cite{nhc}.

The internal Core language of Yhc is PosLambda -- a simple variant of lambda calculus without types, but with source position information. Yhc works by applying basic desugaring transformations, without optimisation. This simplicity ensures the generated PosLambda is close to the original Haskell in its structure. Each top-level function in a source file maps to a top-level function in the generated PosLambda, retaining the same name. However, PosLambda has constructs that have no direct representation in Haskell. For example, there is a FatBar construct \cite{spj:implementation}, used for compiling pattern matches which require fall through behaviour. We have therefore introduced a new Core language to Yhc, to which PosLambda can easily be translated \cite{me:yhc_core}.

The Yhc compiler can generate the Core for a single source file. Yhc can also link in all definitions from all required libraries, producing a single Core file representing a whole program. All function and constructor names are fully qualified, so the linking process simply involves merging the list of functions from each required Core file.

In the process of generating a Core file, Yhc performs several transformations. Haskell's type classes are removed using the dictionary transformation (see \S\ref{sec:dictionary_transformation}). All local functions are lambda lifted, leaving only top-level functions -- ensuring Yhc generated Core does \textit{not} contain any lambda expressions. All constructor applications and primitive applications are fully saturated.


\subsection{The Dictionary Transformation}
\label{sec:dictionary_transformation}

Most transformations in Yhc operate within a single function definition. The only phases which require information about more than one function are type checking and the transformation used to implement type classes \citep{wadler:type_classes}. The dictionary transformation introduces tuples (or \textit{dictionaries}) of methods passed as additional arguments to class-polymorphic functions. Haskell also allows subclassing. For example, |Ord| requires |Eq| for the same type. In such cases the dictionary transformation generates a nested tuple: the |Eq| dictionary is a component of the |Ord| dictionary.

\begin{example}
\label{ex:dictionary}
\begin{code}
f :: Eq alpha => alpha -> alpha -> Bool
f x y = x == y || x /= y
\end{code}

\noindent is translated by Yhc into

\begin{code}
f :: (alpha -> alpha -> Bool, alpha -> alpha -> Bool) -> alpha -> alpha -> Bool
f dict x y = (||) (((==) dict) x y) (((/=) dict) x y)

(==) (a,b) = a
(/=) (a,b) = b
\end{code}

The |Eq| class is implemented as two selector functions, |(==)| and |(/=)|, acting on a method table. For different types of |alpha|, different method tables are provided.
\end{example}

The dictionary transformation is a global transformation. In Example \ref{ex:dictionary} the |Eq| context in |f| not only requires a dictionary to be accepted by |f|; it requires all the callers of |f| to pass a dictionary as first argument. There are alternative approaches to implementing type classes, such as \citet{jones:dictionary_free}, which does not create a tuple of higher order functions. We use the dictionary transformation for simplicity, as it is already implemented within Yhc.

\subsection{Operations on Core}

There are several operations that can be defined on our Core expressions type. We present some of the most useful, which are used later on.

\subsubsection{Substitution}

We define |e[x / y]| to be the capture free substitution of the variable |x| for the expression |y| within the expression |e|. We define |e[x_1,...,x_n / y_1,...,y_n]| to be the simultaneous substitution of each variable |x_i| for each expression |y_i| in |e|.

\begin{example}
\begin{code}
(v + 1)[v / 2]               => 2 + 1
(let v = 3 in v + 1)[v / 2]  => let v = 3 in v + 1
\end{code}
\end{example}

\subsubsection{Variable Classification}

\begin{figure}
\begin{code}
fv :: Expr -> [FreeVar]
fv (v                ) = [v]
fv (c                ) = []
fv (f                ) = []
fv (x ys_            ) = fv x `union` unions (map fv ys_)
fv (\vs_ -> x        ) = fv x \\ vs_
fv (let v = x in y   ) = fv x `union` (fv y \\ v)
fv (case x of alts_  ) = fv x `union` unions (map f alts)
    where f (c vs_ -> y) = fv y \\ vs_
\end{code}
\caption{Free variables of an expression.}
\label{fig:free_variables}
\end{figure}

The variables in the patterns of case expressions, the arguments of lambda abstractions and the bindings of let expressions are \textit{bound}; all other variables are \textit{free}. The set of free variables of an expression |e| is denoted by |fv e|, and can be computed using the function in Figure \ref{fig:free_variables}. Within a function definition all variables must be bound.

In order to avoid accidental variable name clashes while performing transformations, we demand that all variables within a program are unique. All transformations may assume this invariant, and afterwards any repeated bound variables can be assigned fresh names.

\begin{comment}
\subsubsection{The |split| operation}

\begin{figure}
\begin{tabular}{rcl}
|split(v)| & |=| & |(v, [])| \\
|split(c)| & |=| & |(c, [])| \\
|split(f)| & |=| & |(f, [])| \\
|split(x ys_)| & |=| & |(bullet {-"\text{ } \overline{\bullet} "-}, x:ys_)| \\
|split(\vs_ -> x)| & |=| & |(\vs_ -> bullet, x)| \\
|split(let v = x in y)| & |=| & |(let v = bullet in bullet, [x,y])| \\
|split(case x of {p_1 \! -> \! y_1 ; ... ; p_n \! -> \! y_n})| & |=| & |(case {-"\hspace{-1mm}"-} bullet {-"\hspace{-1mm}"-} of {p_1 \! -> \! bullet ; ... ; p_n \! -> \! bullet}, [x, y_1, ... ,y_n])|
\end{tabular}
\caption{The |split| function, returning a spine and all subexpressions.}
\label{fig:split}
\end{figure}

We define the |split| function in Figure \ref{fig:split}, which splits an expression into a pair of its spine and its immediate subexpressions. The $\bullet$ markers in the spine indicate the positions from which subexpressions have been removed. We define the |join| operation to be the inverse of |split|, taking a spine and a list of expressions, and producing an expression.
\end{comment}


\section{Semantics}
\label{sec:semantics}

The evaluation strategy of our Core language is lazy. We specify a reduction to weak-head normal form (WHNF) in Figure \ref{fig:whnf} and a reduction to normal form (NF) in \ref{fig:nf}. The grammar for expressions in WHNF is:

\begin{code}
r  =  c \< xs_ \>  {-" \text{  constructor} "-}
   |  \v -> x      {-" \text{  lambda} "-}
   |  undefined    {-" \text{  bottom/undefined} "-}
\end{code}

The grammar for expression in NF is:

\begin{code}
n  =  c \< ns_ \>  {-" \text{  constructor} "-}
   |  \v -> x      {-" \text{  lambda} "-}
   |  undefined    {-" \text{  bottom/undefined} "-}
\end{code}

\newcommand{\sem}[1]
    {& \begin{array}{c}#1\smallskip\end{array}}
\newcommand{\semm}[2]
    {& \frac{\begin{array}{c}#1\end{array}}
            {\begin{array}{c}#2\smallskip\end{array}}}
\newcommand{\semmm}[3]
    {& \frac{\begin{array}{c}#1\\#2\end{array}}
            {\begin{array}{c}#3\smallskip\end{array}}}

\begin{figure}
\begin{eqnarray}
\sem
    {|c => c \< \>|}
\\ \semmm
    {|_F(f) = x|}
    {|x => r|}
    {|f => r|}
\\ \semm
    {|x => undefined|}
    {|x y => undefined|}
\\ \semm
    {|x => c \< xs_ \>|}
    {|x y => c \< xs_ y \>|}
\\ \semmm
    {|x => \v -> x'|}
    {|x'[v/y] => r|}
    {|x y => r|}
\\ \semm
    {|y[v/x] => r|}
    {|let v = x in y => r|}
\\ \semmm
    {|x => c \< xs_ \>|}
    {|y[vs_ / xs_] => r|}
    {|case x of {... ; c vs_ -> y ; ...} => r|}
\\ \semm
    {|x => undefined|}
    {|case x of alts_ => undefined|}
\end{eqnarray}
\caption{Reduction to weak-head normal form, |(=>)|.}
\label{fig:whnf}
\end{figure}

\begin{figure}
\begin{eqnarray}
\semm
    {|x => undefined|}
    {|x =>* undefined|}
\\ \semm
    {|x => \v -> y|}
    {|x =>* \v -> y|}
\\ \semmm
    {|x => c \< xs_ \>|}
    {|xs_ =>* xs_' |}
    {|x =>* c \< xs_' \>|}
\end{eqnarray}
\caption{Reduction to normal form, |(=>*)|.}
\label{fig:nf}
\end{figure}

Our semantics does not respect sharing, this topic is dealt with in \S\ref{sec:sharing}. The evaluation of a program corresponds to reducing |main| to NF.

We never descend below a bound variable without replacing that variable, therefore any remaining variable would have to be free in the definition of the program, which is not permitted. Consequently, there is no rule for a variable. We choose not evaluate inside a lambda, even on |=>*|, as this would generate a free variable. The |_F| mapping never crashes, as it is checked statically that all mentioned function names exist in the mapping.

The only way an undefined value may be initially generated is by a call to the error function in the program, i.e. |_F(error) = undefined|. If a computation forces a |undefined| value, in the scrutinee of a case or the first argument of an application, the |undefined| is propagated.

There is no rule for case of a lambda abstraction, as this situation is not permitted by the type system. Similarly, the type system plus the exhaustiveness of case branches means that if the scrutinee successfully evaluates, it will match exactly one alternative.

\subsection{Simplification Rules}

\begin{figure}
\renewcommand{\f}[2]{\vspace{-7mm} #2 & (#1) \\}

\begin{flushright}
\begin{tabular}{p{8cm}r}
\f{case-con}{
\begin{code}
case c xs_ of {... ; c vs_ -> y ; ...}
    => let vs_ = xs_ in y
\end{code}}

\f{lam-app}{
\begin{code}
(\v -> x) y
    => let v = y in x
\end{code}}

\f{case-app}{
\begin{code}
(case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) z
    => case x of {p_1 -> y_1 z ; ... ; p_n -> y_n z}
\end{code}}

\f{let-app}{
\begin{code}
(let v = x in y) z
    => let v = x in y z
\end{code}}

\f{let-case}{
\begin{code}
let v = x in (case y of {p_1 -> y_1 ; ... ; p_n -> y_n})
    => case y of  {  p_1  -> let v = x in y_1
                  ;  ...
                  ;  p_n  -> let v = x in y_n}
    where v {-" \hbox{is not used in } "-} y
\end{code}}

\f{case-case}{
\begin{code}
case (case x of {p_1 -> y_1 ; ... ; p_n -> y_n}) of alts_
    => case x of  {  p_1  -> case y_1 of alts_
                  ;  ...
                  ;  p_n  -> case y_n of alts_ }
\end{code}}

\f{let}{
\begin{code}
let v = x in y
    => y[v/x]
    where x {-" \hbox{is used once in } "-} y
\end{code}}
\end{tabular}
\end{flushright}
\caption{Simplification rules.}
\label{fig:simplify}
\end{figure}

We present several simplification rules in Figure \ref{fig:simplify}, which can be applied to our Core language. These rules are standard and would be applied by any optimising compiler. All the rules preserve both the semantics and the sharing behaviour of an expression.

The case-con and lam-app rules simply follow the semantics, using let expressions to preserve the sharing. The case-app, let-case and case-case rules move outer expressions over an inner case expression, duplicating the outer expression in each alternative. The let-app rule moves an application over an inner let expression. The let rule substitutes let expressions where the bound variable is used only once, and therefore no loss of sharing is possible.


\section{Sharing}
\label{sec:sharing}

The semantics presented give the correct answer, and model laziness correctly, but do not model the sharing present in Haskell. This section informally discusses the relevant sharing properties of Haskell. In general, any optimisation must take account of sharing, but semantic analysis can sometimes ignore the effects of sharing.

The sharing present in Haskell is not specified in the Haskell Report \cite{haskell}, but is covered in other papers.

\subsection{Let bindings}

A let expression introduces \textit{sharing} of the computational result of expressions. Consider the expression:

\begin{example}
\begin{code}
let x = f 1
in x + x
\end{code}

The semantic rules for evaluating this expression result in:

\begin{code}
(x + x)[x / f 1]
(f 1 + f 1)
\end{code}

In the semantics, the expression |f 1| will be reduced twice. However, the real operational semantics would evaluate |f 1| only once. The first time the value of |x| was demanded, |f 1| would be evaluated to WHNF, and bound to |x|. Any successive examinations of |x| would return immediately, pointing at the same result.
\end{example}

\begin{figure}
\begin{code}
occurs :: VarName -> Expr -> Int
occurs v (v'                ) = if v == v' then 1 else 0
occurs v (c                 ) = 0
occurs v (f                 ) = 0
occurs v (x ys_             ) = occurs v x + sum (map (occurs v) ys_)
occurs v (\vs_ -> x         ) = occurs v x
occurs v (let v' = x in y   ) = if v == v' then 0 else occurs v x + occurs v y
occurs v (case x of alts_   ) = occurs v x + maximum (map f alts_)
    where f (c vs_ -> y) = if v `elem` vs_ then 0 else occurs v y

linear :: VarName -> Expr -> Bool
linear v x = occurs v x <= 1
\end{code}
\caption{Linear variables within an expression.}
\label{fig:linear}
\end{figure}

In general, the substitution of a bound variable for the associated expression may cause duplicate computation to be formed. However, in some circumstances, duplicate computation can be guaranteed not to occur. If a bound variable can be used at most once in an expression, it is said to be \textit{linear}, and substitution can be performed. A variable is linear if it is used at most once, i.e. occurs at most once down each possible flow of control, and can be computed following Figure \ref{fig:linear}.

\subsection{Recursive let bindings}

A recursive let binding is one where the bound variables are in scope during the computation of their associated expression. In the Haskell language, let bindings can be \textit{recursive}. In other languages, such as ML \cite{ml}, recursive let binding are made explicit with the |letrec| keyword. One common use of |letrec| in Haskell is the |repeat| function:

\begin{example}
\begin{code}
repeat x =  letrec xs = x : xs
            in xs
\end{code}

Here the variable |xs| is both defined and referenced in the binding. Given the application |repeat 1|, regardless of how much of the list is examined, the program will only ever create one single cons cell. This construct effectively ties a loop in the memory.
\end{example}

Our Core language does not allow recursive let bindings, for reasons of simplicity. If there is a |letrec| bound to a function, it will be removed by lambda lifting. The only remaining case is a value letrec. We can remove these letrec's by inserting a dummy argument. Our algorithm is:

\begin{enumerate}
\item For each recursive let binding, replace it with a lambda containing a dummy argument. For each reference to a recursive let variable, replace it with an application with the same dummy variable.
\item Perform lambda lifting as normal.
\item Optionally, remove the inserted |dummy| markers.
\end{enumerate}

Applying this to our example from before, we first add dummy arguments:

\begin{code}
repeat x =  letrec xs = \dummy ->  x : xs dummy
            in xs dummy
\end{code}

Then we lambda lift:

\begin{code}
repeat x = f caf x

f caf x = x : f caf x
\end{code}

Optionally, we can remove the |dummy| arguments:

\begin{code}
repeat x = f x

f x = x : f x
\end{code}

In the case where the |dummy| argument is not the only argument to a function, removal of the |caf| annotation to this function is simplifies the code without changing the space behaviour. In the case where it is the only argument, removing the |dummy| argument will result in a CAF, which will be computed only once at runtime -- changing the space behaviour.

In the |repeat| example we have lost the sharing, but in this case we occur only a constant time overhead compared to the original value. Because we are working in a referentially transparent language, to examine the $n$th element of the list generated by |repeat| will take at least $O(n)$. To generate those elements using the first algorithm will take $O(1)$, but using the second version will take $O(n)$. If the elements are not examined they will not be required due to lazy evaluation, therefore it is impossible to harm.

It is possible to engineer examples where the recursive let removal does cause a serious slowdown:

\subsection{Constant Applicative Form}

A Constant Applicative Form (CAF) is a top level definition of zero arity. In Haskell, CAFs are computed at most once per program run, and retained as long as references to them remain.

\begin{example}
\begin{code}
caf = expensive

main = caf + caf
\end{code}

While the semantics presented will lookup the expression bound to |caf| twice, and compute it twice, a real compiler will only perform |expensive| once.
\end{example}

If a general function is inlined, this will not dramatically change the runtime behaviour of a program. If a CAF is inlined, this may have adverse effects on the performance. There are two alternatives to removing CAF's and their special space behaviour.

\subsubsection{Lifting to |letrec|}

One translation is to move the bindings to the first executed function, and pass them onwards throughout the program.

\begin{code}
main' =  letrec caf = expensive
         in main caf

main caf = caf + caf
\end{code}

In this example, we arrange for |main'| to be executed first, bind the CAF's, and pass them onwards to the functions that require them. This translation preserves the space behaviour

\subsubsection{Inserting Dummy Arguments}

A CAF can be unshared by adding a dummy argument to the definition, and passing a dummy argument to all applications. Taking the previous example:

\begin{code}
caf dummy = expensive

main = caf dummy + caf dummy
\end{code}

Now |caf| is no longer a CAF, and will be recomputed each time. The |dummy| value passed by main can be any value, such as |()|.



\subsection{Common Subexpression Elimination}

\cite{chitil:cse}

